import time
import json
import re
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã€‘Secretsèª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¤‰æ•°
YEAR = "2025"
PLACE_CODE = "11"  # 10:å¤§äº•, 11:å·å´, 12:èˆ¹æ©‹, 13:æµ¦å’Œ
MONTH = "12"
DAY = "04"

def set_race_params(year, place_code, month, day):
    global YEAR, PLACE_CODE, MONTH, DAY
    YEAR = str(year)
    PLACE_CODE = str(place_code).zfill(2)
    MONTH = str(month).zfill(2)
    DAY = str(day).zfill(2)

# ==================================================
# Supabase & Helper
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if not supabase: return
    data = {
        "year": str(year),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "month": str(month),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }
    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)

# ==================================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ç¾¤
# ==================================================

# 1. ãƒ¬ãƒ¼ã‚¹IDä¸€è¦§ã‚’å–å¾—ï¼ˆæ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
def fetch_race_ids_from_schedule(driver, year, month, day, target_place_code):
    """
    æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®æ—¥ç¨‹ãƒšãƒ¼ã‚¸(nittei)ã‹ã‚‰ã€å¯¾è±¡ç«¶é¦¬å ´(place_code)ã®å…¨ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã™ã‚‹
    """
    date_str = f"{year}{month}{day}"
    # URLæœ«å°¾ã®10ã¯å›ºå®š
    url = f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10"
    
    st.info(f"ğŸ“… æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ä¸­... ({url})")
    driver.get(url)
    time.sleep(1)
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    race_ids = []
    
    # ãƒšãƒ¼ã‚¸å†…ã®ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDãƒ‘ã‚¿ãƒ¼ãƒ³(16æ¡)ã‚’æ¢ã™
    # IDã®5-6æ¡ç›®ãŒ place_code ã¨ä¸€è‡´ã™ã‚‹ã‚‚ã®ã ã‘ã‚’æŠ½å‡º
    
    seen = set()
    for a in soup.find_all("a", href=True):
        href = a['href']
        match = re.search(r'(\d{16})', href)
        if match:
            rid = match.group(1)
            # IDã®æ§‹é€ : YYYY(4) + PLACE(2) + ...
            # target_place_code (ä¾‹: "11") ãŒ IDã® 5,6æ–‡å­—ç›®ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            if rid[6:8] == target_place_code:
                if rid not in seen:
                    race_ids.append(rid)
                    seen.add(rid)
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·é †(IDã®å¾Œã‚ã®æ–¹ã«ã‚ã‚‹Rç•ªå·ã§ã‚½ãƒ¼ãƒˆ)
    race_ids.sort()
    
    if not race_ids:
        st.warning(f"âš ï¸ æŒ‡å®šã—ãŸç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰({target_place_code})ã®ãƒ¬ãƒ¼ã‚¹IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–‹å‚¬ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        st.success(f"âœ… {len(race_ids)} ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        
    return race_ids

# 2. é¨æ‰‹æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£æ¸ˆã¿ï¼šãƒªãƒ³ã‚¯ãªã—å¯¾å¿œç‰ˆï¼‰
def parse_syutuba_jockey(html: str):
    soup = BeautifulSoup(html, "html.parser")
    jockey_info = {}
    
    # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ç‰ˆ(SP)ã®å‡ºé¦¬è¡¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
    table = soup.find("table", class_="syutuba_sp")
    if not table:
        return {}

    # tbodyã®ä¸­èº«ã ã‘ã‚’è¦‹ã‚‹
    tbody = table.find("tbody")
    if not tbody:
        return {}
        
    rows = tbody.find_all("tr")
    
    for row in rows:
        # 1. é¦¬ç•ªã®å–å¾—
        tds = row.find_all("td")
        if not tds:
            continue
            
        # 1åˆ—ç›®ãŒé¦¬ç•ªï¼ˆãƒ†ã‚­ã‚¹ãƒˆãŒæ•°å­—ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
        umaban_text = tds[0].get_text(strip=True)
        if not umaban_text.isdigit():
            continue
            
        umaban = umaban_text

        # 2. é¨æ‰‹æƒ…å ±ã®å–å¾—
        # <td class="left"> ã®ä¸­ã«ã‚ã‚‹ <p class="kisyu"> ã‚’æ¢ã™
        kisyu_p = row.find("p", class_="kisyu")
        
        if kisyu_p:
            name = ""
            is_change = False
            
            # ã‚¢ãƒ³ã‚«ãƒ¼ã‚¿ã‚°(a)ãŒã‚ã‚‹ã‹ç¢ºèª
            anchor = kisyu_p.find("a")
            
            if anchor:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³A: ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆ
                name = anchor.get_text(strip=True)
                # ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®š (aã‚¿ã‚°ã®ä¸­ã€ã¾ãŸã¯pã‚¿ã‚°ç›´ä¸‹ã«strongãŒã‚ã‚‹ã‹)
                if anchor.find("strong") or kisyu_p.find("strong"):
                    is_change = True
            else:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³B: ãƒªãƒ³ã‚¯ãŒãªã„å ´åˆï¼ˆã“ã“ã‚’ä¿®æ­£ï¼‰
                # pã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥å–å¾—
                name = kisyu_p.get_text(strip=True)
                # ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®š
                if kisyu_p.find("strong"):
                    is_change = True
            
            # åå‰ãŒå–å¾—ã§ãã¦ã„ã‚Œã°ä¿å­˜
            if name:
                jockey_info[umaban] = {"name": name, "is_change": is_change}
            
    return jockey_info

# 3. è«‡è©±ãƒ»èª¿æ•™ãƒ»ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®ãƒ‘ãƒ¼ã‚¹
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle: return {}
    
    racemei = racetitle.find("div", class_="racemei")
    race_name = racemei.find_all("p")[1].get_text(strip=True) if racemei and len(racemei.find_all("p")) >= 2 else ""
    
    sub = racetitle.find("div", class_="racetitle_sub")
    cond = sub.find_all("p")[1].get_text(" ", strip=True) if sub and len(sub.find_all("p")) >= 2 else ""
    
    return {"race_name": race_name, "cond": cond}

def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    danwa_dict = {}
    # è«‡è©±ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã«åˆã‚ã›ã¦å–å¾—
    table = soup.find("table", class_="danwa")
    if table and table.tbody:
        current_uma = None
        for row in table.tbody.find_all("tr"):
            uma_td = row.find("td", class_="umaban")
            if uma_td:
                current_uma = uma_td.get_text(strip=True)
                continue
            txt_td = row.find("td", class_="danwa")
            if txt_td and current_uma:
                danwa_dict[current_uma] = txt_td.get_text(strip=True)
                current_uma = None
    return danwa_dict

def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    tables = soup.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows: continue
        
        h_row = rows[0]
        uma_td = h_row.find("td", class_="umaban")
        name_td = h_row.find("td", class_="kbamei")
        if not uma_td or not name_td: continue
        
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        
        # çŸ­è©•ã¨è©³ç´°
        tanpyo = h_row.find("td", class_="tanpyo").get_text(strip=True) if h_row.find("td", class_="tanpyo") else ""
        detail = rows[1].get_text(" ", strip=True) if len(rows) > 1 else ""
        
        cyokyo_dict[umaban] = f"ã€é¦¬åã€‘{bamei} ã€çŸ­è©•ã€‘{tanpyo} ã€è©³ç´°ã€‘{detail}"
    return cyokyo_dict

# ==================================================
# Dify é€£æº
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
        return
    
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=300)
        for line in res.iter_lines():
            if line:
                decoded = line.decode('utf-8')
                if decoded.startswith("data:"):
                    try:
                        data = json.loads(decoded.replace("data: ", ""))
                        if data.get("event") == "workflow_finished":
                            out = data.get("data", {}).get("outputs", {})
                            yield "".join([v for v in out.values() if isinstance(v, str)])
                        elif "answer" in data:
                            yield data.get("answer", "")
                    except: pass
    except Exception as e:
        yield f"âš ï¸ API Error: {str(e)}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
def run_all_races(target_races=None):
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(PLACE_CODE, "åœ°æ–¹")

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        # 1. ãƒ­ã‚°ã‚¤ãƒ³
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(1)

        # 2. æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰IDãƒªã‚¹ãƒˆã‚’å–å¾—
        race_ids = fetch_race_ids_from_schedule(driver, YEAR, MONTH, DAY, PLACE_CODE)
        
        if not race_ids:
            return

        # 3. å„ãƒ¬ãƒ¼ã‚¹ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
        for i, race_id in enumerate(race_ids):
            race_num = i + 1  # ãƒªã‚¹ãƒˆé †ï¼ãƒ¬ãƒ¼ã‚¹é †ã¨ä»®å®š
            
            # æŒ‡å®šã•ã‚ŒãŸãƒ¬ãƒ¼ã‚¹ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
            if target_races is not None and race_num not in target_races:
                continue

            race_num_str = f"{race_num:02}"
            
            st.markdown(f"### {place_name} {race_num}R (ID: {race_id})")
            status_area = st.empty()
            result_area = st.empty()
            
            try:
                status_area.info("ğŸ“¡ ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                
                # A. è«‡è©±ãƒšãƒ¼ã‚¸ (ã“ã“ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚‚å–ã‚‹)
                driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}")
                html_danwa = driver.page_source
                race_meta = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)
                
                # B. å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ (é¨æ‰‹æƒ…å ±) - /1/ç„¡ã—æ³¨æ„
                driver.get(f"https://s.keibabook.co.jp/chihou/syutuba/{race_id}")
                jockey_dict = parse_syutuba_jockey(driver.page_source)
                
                # C. èª¿æ•™ãƒšãƒ¼ã‚¸
                driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{race_id}")
                cyokyo_dict = parse_cyokyo(driver.page_source)
                
                # ãƒ‡ãƒ¼ã‚¿çµåˆ
                merged_text = []
                all_uma = sorted(list(set(list(danwa_dict.keys()) + list(cyokyo_dict.keys()) + list(jockey_dict.keys()))), 
                                 key=lambda x: int(x) if x.isdigit() else 99)
                
                for uma in all_uma:
                    j = jockey_dict.get(uma, {"name": "ä¸æ˜", "is_change": False})
                    d = danwa_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                    c = cyokyo_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                    
                    alert = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if j["is_change"] else ""
                    merged_text.append(f"â–¼[é¦¬ç•ª{uma}] {j['name']} {alert}\n è«‡è©±: {d}\n èª¿æ•™: {c}")

                if not merged_text:
                    status_area.warning("ãƒ‡ãƒ¼ã‚¿ãªã—ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                prompt = (
                    f"ãƒ¬ãƒ¼ã‚¹å: {race_meta.get('race_name','')}\n"
                    f"æ¡ä»¶: {race_meta.get('cond','')}\n\n"
                    "ä»¥ä¸‹ã¯å‡ºèµ°å…¨é ­ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆé¨æ‰‹ã€èª¿æ•™å¸«ã€è«‡è©±ã€èª¿æ•™ï¼‰ã§ã™ã€‚\n"
                    + "\n".join(merged_text)
                )
                
                # AIåˆ†æ
                status_area.info("ğŸ¤– AIåˆ†æä¸­...")
                full_ans = ""
                for chunk in stream_dify_workflow(prompt):
                    full_ans += chunk
                    result_area.markdown(full_ans + "â–Œ")
                
                result_area.markdown(full_ans)
                status_area.success("å®Œäº†")
                save_history(YEAR, PLACE_CODE, place_name, MONTH, DAY, race_num_str, race_id, full_ans)

            except Exception as e:
                status_area.error(f"Error: {e}")
                
            st.write("---")

    finally:
        driver.quit()
