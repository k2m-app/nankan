import time
import json
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ç­‰ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹ï¼‰
YEAR = "2025"
PLACE_CODE = "11" # å·å´ãªã©
MONTH = "12"
DAY = "18"

def set_race_params(year, place_code, month, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, PLACE_CODE, MONTH, DAY
    YEAR = str(year)
    PLACE_CODE = str(place_code).zfill(2)
    MONTH = str(month).zfill(2)
    DAY = str(day).zfill(2)

# ==================================================
# Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    """history ãƒ†ãƒ¼ãƒ–ãƒ«ã« 1 ãƒ¬ãƒ¼ã‚¹åˆ†ã®äºˆæƒ³ã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": "",          
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),    
        "month": str(month), 
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)

# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹é–¢æ•°ç¾¤
# ==================================================

def parse_race_info(html: str):
    """ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }

def parse_danwa_comments(html: str):
    """å©èˆã®è©±ï¼ˆè«‡è©±ï¼‰ã‚’ãƒ‘ãƒ¼ã‚¹"""
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}
    danwa_dict = {}
    current = None
    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        if uma_td:
            current = uma_td.get_text(strip=True)
            continue
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current:
            danwa_dict[current] = danwa_td.get_text(strip=True)
            current = None
    return danwa_dict

def parse_cyokyo(html: str):
    """èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹"""
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    section = None
    h2 = soup.find("h2", string=lambda s: s and "èª¿æ•™" in s)
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup
    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows:
            continue
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")
        if not uma_td or not name_td:
            continue
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = ""
        if detail_row:
            detail_text = detail_row.get_text(" ", strip=True)
        final_text = f"ã€é¦¬åã€‘{bamei}ï¼ˆé¦¬ç•ª{umaban}ï¼‰ ã€çŸ­è©•ã€‘{tanpyo} ã€èª¿æ•™è©³ç´°ã€‘{detail_text}"
        cyokyo_dict[umaban] = final_text
    return cyokyo_dict

def parse_syutuba_jockey(html: str):
    """
    å‡ºé¦¬è¡¨ï¼ˆ/chihou/syutuba/ï¼‰ã‹ã‚‰é¨æ‰‹æƒ…å ±ã¨ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®šã‚’å–å¾—
    """
    soup = BeautifulSoup(html, "html.parser")
    jockey_info = {}
    
    # ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®ã‚¹ãƒãƒ›ç‰ˆæ§‹é€ ã«å¯¾å¿œï¼š<div class="section"> å†…ã«å„é¦¬ã®æƒ…å ±ãŒã‚ã‚‹
    sections = soup.find_all("div", class_="section")
    
    for sec in sections:
        # é¦¬ç•ªã®å–å¾—
        umaban_div = sec.find("div", class_="umaban")
        if not umaban_div:
            continue
        umaban = umaban_div.get_text(strip=True)
        
        # é¨æ‰‹åã®å–å¾—
        # <p class="kisyu"><strong>ç”ºç”°ç›´</strong></p> ã®ã‚ˆã†ãªæ§‹é€ ã‚’æ¢ã™
        kisyu_p = sec.find("p", class_="kisyu")
        if kisyu_p:
            # <strong>ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã€ä¹—ã‚Šæ›¿ã‚ã‚Šã¨åˆ¤å®š
            is_change = True if kisyu_p.find("strong") else False
            
            # ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé¨æ‰‹åï¼‰ã®ã¿æŠ½å‡º
            name = kisyu_p.get_text(strip=True)
            
            jockey_info[umaban] = {
                "name": name,
                "is_change": is_change
            }
            
    return jockey_info

BASE_URL = "https://s.keibabook.co.jp"

def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/chihou/cyokyo/1/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        return {}
    html = driver.page_source
    return parse_cyokyo(html)

def fetch_syutuba_dict(driver, race_id: str):
    """å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦é¨æ‰‹æƒ…å ±ã‚’è¿”ã™"""
    url = f"{BASE_URL}/chihou/syutuba/1/{race_id}"
    driver.get(url)
    try:
        # é¦¬ç•ªã‚¯ãƒ©ã‚¹ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CLASS_NAME, "umaban"))
        )
    except Exception:
        return {}
    html = driver.page_source
    return parse_syutuba_jockey(html)

# ==================================================
# â˜…Dify ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–¢æ•°
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300, 
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                if decoded_line.startswith("data:"):
                    json_str = decoded_line.replace("data: ", "")
                    try:
                        data = json.loads(json_str)
                        event = data.get("event")
                        if event in ["workflow_started", "node_started", "node_finished"]:
                            yield ""
                            continue
                        chunk = data.get("answer", "")
                        if chunk:
                            yield chunk
                        if event == "workflow_finished":
                            outputs = data.get("data", {}).get("outputs", {})
                            if outputs:
                                found_text = ""
                                for key, value in outputs.items():
                                    if isinstance(value, str):
                                        found_text += value + "\n"
                                if found_text:
                                    yield found_text
                                else:
                                    yield f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Raw: {outputs}"
                    except json.JSONDecodeError:
                        pass
                    except Exception as e:
                        yield f"âš ï¸ Parse Error: {str(e)}"

    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†: å…¨ãƒ¬ãƒ¼ã‚¹å®Ÿè¡Œ
# ==================================================
def run_all_races(target_races=None):
    
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    place_names = {
        "10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ",
        "30": "åœ’ç”°", "42": "é–€åˆ¥", "19": "ç¬ æ¾", "34": "åå¤å±‹",
        "20": "é‡‘æ²¢", "29": "æ°´æ²¢", "33": "ç››å²¡", "58": "å¸¯åºƒ",
        "26": "é«˜çŸ¥", "23": "ä½è³€"
    }
    place_name = place_names.get(PLACE_CODE, "åœ°æ–¹")

    # Selenium è¨­å®š
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        # --- 1. ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç† ---
        st.info("ğŸ”‘ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã¸ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        
        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()
        
        time.sleep(2)
        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã€‚ãƒ¬ãƒ¼ã‚¹åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # --- 2. å„ãƒ¬ãƒ¼ã‚¹å‡¦ç† ---
        for r in race_numbers:
            race_num_str = f"{r:02}"
            
            # URLç”Ÿæˆ
            date_str = f"{MONTH}{DAY}"
            race_id = f"{YEAR}11{PLACE_CODE}01{race_num_str}{date_str}"

            st.markdown(f"### {place_name} {r}R (ID: {race_id})")
            
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                # ==========================
                # Phase A: ãƒ‡ãƒ¼ã‚¿åé›†ä¸­
                # ==========================
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")
                
                # A-1. å©èˆã‚³ãƒ¡ãƒ³ãƒˆ (è«‡è©±)
                url_danwa = f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}"
                driver.get(url_danwa)
                time.sleep(1)
                html_danwa = driver.page_source
                
                race_info = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)

                # A-2. å‡ºé¦¬è¡¨ (é¨æ‰‹ãƒ»ä¹—ã‚Šæ›¿ã‚ã‚Š)
                # ã“ã“ã§æ–°ã—ã„é–¢æ•°ã‚’ä½¿ç”¨
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                # A-3. èª¿æ•™
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-4. ãƒ‡ãƒ¼ã‚¿çµåˆ
                # å…¨ã¦ã®è¾æ›¸ã‹ã‚‰é¦¬ç•ªã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                all_uma = sorted(
                    list(set(list(danwa_dict.keys()) + list(cyokyo_dict.keys()) + list(syutuba_dict.keys()))),
                    key=lambda x: int(x) if x.isdigit() else 99
                )

                merged = []
                for uma in all_uma:
                    d_txt = danwa_dict.get(uma, 'ï¼ˆæƒ…å ±ãªã—ï¼‰')
                    c_txt = cyokyo_dict.get(uma, 'ï¼ˆæƒ…å ±ãªã—ï¼‰')
                    
                    # é¨æ‰‹æƒ…å ±ã®å–å¾—
                    j_info = syutuba_dict.get(uma, {"name": "ä¸æ˜", "is_change": False})
                    j_name = j_info["name"]
                    # ä¹—ã‚Šæ›¿ã‚ã‚Šãªã‚‰ãƒãƒ¼ã‚¯ã‚’ã¤ã‘ã‚‹
                    change_alert = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if j_info["is_change"] else "ã€ç¶™ç¶šé¨ä¹—ã€‘"

                    text = (
                        f"â–¼[é¦¬ç•ª{uma}]\n"
                        f"  ã€é¨æ‰‹ã€‘ {j_name} {change_alert}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_txt}\n"
                        f"  ã€èª¿æ•™ã€‘ {c_txt}\n"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning(f"âš ï¸ {place_name} {r}R: ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                race_header_lines = []
                if race_info["date_meet"]: race_header_lines.append(race_info["date_meet"])
                if race_info["race_name"]: race_header_lines.append(race_info["race_name"])
                if race_info["cond1"]: race_header_lines.append(race_info["cond1"])
                if race_info["course_line"]: race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)
                
                # å—é–¢ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®URLï¼ˆå›ºå®šã¾ãŸã¯å‹•çš„ï¼‰
                # ã“ã“ã§ã¯æ±ç”¨çš„ãªãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸURLã‚’æç¤º
                nankan_leading_url = "https://www.nankankeiba.com/leading_kis/180000000003011.do"
                
                full_text = (
                    "â– å½¹å‰²\n"
                    "ã‚ãªãŸã¯å—é–¢æ±ç«¶é¦¬ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«äºˆæƒ³å®¶ã§ã™ã€‚\n\n"
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    "â– æŒ‡ç¤º\n"
                    f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€{place_name}{r}Rã®å±•é–‹ã¨æ¨å¥¨é¦¬ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚\n"
                    "ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š\n"
                    "1. ã€Œä¹—ã‚Šæ›¿ã‚ã‚Šã€ãŒç™ºç”Ÿã—ã¦ã„ã‚‹é¦¬ã«ã¤ã„ã¦ã¯ã€ãã®ãƒ—ãƒ©ã‚¹/ãƒã‚¤ãƒŠã‚¹å½±éŸ¿ã‚’è€ƒå¯Ÿã™ã‚‹ã“ã¨ã€‚\n"
                    "2. é¨æ‰‹ã®è©²å½“ã‚³ãƒ¼ã‚¹é©æ€§ã«ã¤ã„ã¦ã¯ã€ä¸€èˆ¬çš„ãªå‚¾å‘ã‚„å—é–¢ç«¶é¦¬ã®ã‚»ã‚ªãƒªãƒ¼ã‚’åŠ å‘³ã™ã‚‹ã“ã¨ï¼ˆä»¥ä¸‹ã®URLã®ãƒ‡ãƒ¼ã‚¿ç­‰ã‚’çŸ¥è­˜ã¨ã—ã¦å‚ç…§ï¼‰ã€‚\n"
                    f"   å‚è€ƒURL: {nankan_leading_url}\n\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                # ==========================
                # Phase B: AIæ€è€ƒä¸­
                # ==========================
                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")
                
                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")
                
                # ==========================
                # Phase C: å®Œäº†
                # ==========================
                result_area.markdown(full_answer)
                
                if full_answer:
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    save_history(
                        YEAR, PLACE_CODE, place_name, MONTH, DAY,
                        race_num_str, race_id, full_answer
                    )
                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                status_area.error(err_msg)
            
            st.write("---")

    finally:
        driver.quit()
