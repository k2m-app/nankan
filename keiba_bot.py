import time
import json
import re
import requests
import streamlit as st

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã€‘Secretsèª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    try:
        return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
    except Exception as e:
        print("Supabase client error:", e)
        return None

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if not supabase:
        return
    data = {
        "year": str(year),
        "place_code": str(place_code),
        "place_name": place_name,
        "month": str(month).zfill(2),
        "day": str(day).zfill(2),
        "race_num": str(race_num_str),
        "race_id": str(race_id),
        "output_text": ai_answer,
    }
    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)

# ==================================================
# Selenium Driverï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ç”¨ï¼‰
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1400,2200")
    return webdriver.Chrome(options=options)

def login_keibabook(driver: webdriver.Chrome, wait: WebDriverWait):
    driver.get("https://s.keibabook.co.jp/login/login")
    wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
    driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
    driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
    time.sleep(1)

# ==================================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼šæ—¥ç¨‹â†’ãƒ¬ãƒ¼ã‚¹IDä¸€è¦§ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰
# ==================================================
def fetch_race_ids_from_schedule(driver, year, month, day, target_place_code):
    """
    æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ã€ŒæŒ‡å®šç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ã€ã®ãƒ¬ãƒ¼ã‚¹ID(16æ¡)ã‚’æ‹¾ã†ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰
    """
    date_str = f"{year}{month}{day}"
    url = f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10"

    st.info(f"ğŸ“… æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ä¸­... ({url})")
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "a")))
    except:
        pass

    soup = BeautifulSoup(driver.page_source, "html.parser")
    race_ids = []
    seen = set()

    for a in soup.find_all("a", href=True):
        href = a["href"]
        m = re.search(r"(\d{16})", href)
        if not m:
            continue
        rid = m.group(1)

        # rid[6:8] ãŒç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯å´ï¼‰
        if rid[6:8] == target_place_code:
            if rid not in seen:
                race_ids.append(rid)
                seen.add(rid)

    race_ids.sort()
    if not race_ids:
        st.warning(f"âš ï¸ æŒ‡å®šã—ãŸç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰({target_place_code})ã®ãƒ¬ãƒ¼ã‚¹IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.success(f"âœ… {len(race_ids)} ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return race_ids

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šãƒ¬ãƒ¼ã‚¹æƒ…å ±/è«‡è©±/èª¿æ•™ï¼ˆå¾“æ¥é€šã‚Šï¼‰
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {}

    racemei = racetitle.find("div", class_="racemei")
    p_tags = racemei.find_all("p") if racemei else []
    race_name = ""
    if len(p_tags) >= 2:
        race_name = p_tags[1].get_text(strip=True)
    elif len(p_tags) == 1:
        race_name = p_tags[0].get_text(strip=True)

    sub = racetitle.find("div", class_="racetitle_sub")
    sub_p = sub.find_all("p") if sub else []
    cond = sub_p[1].get_text(" ", strip=True) if len(sub_p) >= 2 else ""

    return {"race_name": race_name, "cond": cond}

def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    danwa_dict = {}
    table = soup.find("table", class_="danwa")

    if table and table.tbody:
        current_uma = None
        for row in table.tbody.find_all("tr"):
            uma_td = row.find("td", class_="umaban")
            if uma_td:
                current_uma = uma_td.get_text(strip=True)
                continue

            txt_td = row.find("td", class_="danwa")
            if txt_td and current_uma:
                danwa_dict[current_uma] = txt_td.get_text(strip=True)
                current_uma = None

    return danwa_dict

def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    tables = soup.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows:
            continue

        h_row = rows[0]
        uma_td = h_row.find("td", class_="umaban")
        name_td = h_row.find("td", class_="kbamei")
        if not uma_td or not name_td:
            continue

        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)

        tanpyo_elem = h_row.find("td", class_="tanpyo")
        tanpyo = tanpyo_elem.get_text(strip=True) if tanpyo_elem else ""
        detail = rows[1].get_text(" ", strip=True) if len(rows) > 1 else ""

        cyokyo_dict[umaban] = f"ã€é¦¬åã€‘{bamei} ã€çŸ­è©•ã€‘{tanpyo} ã€è©³ç´°ã€‘{detail}"

    return cyokyo_dict

# ==================================================
# åœ°æ–¹ç«¶é¦¬å…¬å¼ï¼ˆkeiba.go.jpï¼‰ï¼šDebaTableSmall ã‚’å …ç‰¢ãƒ‘ãƒ¼ã‚¹
# ==================================================
_KEIBAGO_UA = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
}

def _norm_name(s: str) -> str:
    s = (s or "").strip()
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s)
    # æ¸›é‡è¨˜å·ã‚’é™¤å»ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    s = s.replace("â–²", "").replace("â–³", "").replace("â˜†", "").replace("â—‡", "")
    return s.strip()

def fetch_keibago_debatable_small(year: str, month: str, day: str, race_no: int, baba_code: str):
    """
    DebaTableSmall ã‹ã‚‰
      - ãƒ¬ãƒ¼ã‚¹è¦‹å‡ºã—ï¼ˆãƒšãƒ¼ã‚¸å…ˆé ­ï¼‰
      - {é¦¬ç•ª: {horse, jockey, trainer, prev_jockey, is_change}}
    ã‚’è¿”ã™

    ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®šï¼š
      - ãƒ–ãƒ­ãƒƒã‚¯å†…ã§æœ€åˆã«å‡ºç¾ã™ã‚‹ã€Œå‰èµ°ã€ã®é¨æ‰‹åï¼ˆä¾‹: '12/14 8äºº â–²å°é‡ä¿Š 53.0' ã®å°é‡ä¿Šï¼‰
      - ç¾åœ¨é¨æ‰‹ã¨ä¸ä¸€è‡´ãªã‚‰ is_change=True
    """
    # YYYY/MM/DD ã‚’ URLã«å…¥ã‚Œã‚‹
    date_str = f"{year}/{str(month).zfill(2)}/{str(day).zfill(2)}"
    url = (
        "https://www.keiba.go.jp/KeibaWeb/TodayRaceInfo/DebaTableSmall"
        f"?k_raceDate={requests.utils.quote(date_str)}&k_raceNo={race_no}&k_babaCode={baba_code}"
    )

    r = requests.get(url, headers=_KEIBAGO_UA, timeout=25)
    r.raise_for_status()
    r.encoding = r.apparent_encoding or "utf-8"

    soup = BeautifulSoup(r.text, "html.parser")
    text = soup.get_text("\n", strip=True)
    lines = [ln.strip() for ln in text.split("\n") if ln.strip()]

    # è¦‹å‡ºã—ï¼ˆå…ˆé ­æ•°è¡Œã‚’é€£çµï¼‰
    header = " / ".join(lines[:5])

    # 1è¡Œç›®ã®é¦¬ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ï¼ˆä¾‹: "1 1 ã‚ªãƒ«ãƒ•ã‚§ãƒ¼ãƒ´ãƒ«   ç‰ 3"ï¼‰
    start_re = re.compile(r"^(\d+)\s+(\d+)\s+(.+)$")

    # å‰èµ°è¡Œã®é¨æ‰‹æŠ½å‡ºï¼ˆä¾‹: "12/14 8äºº â–²å°é‡ä¿Š 53.0"ï¼‰
    prev_jockey_re = re.compile(r"^\d{1,2}/\d{1,2}\s+\d+äºº\s+([â˜†â–²â–³â—‡]?\S+)\s+\d{1,2}\.\d$")

    horses = {}
    i = 0
    cur = None

    def _finalize(cur_obj):
        if not cur_obj:
            return
        umaban = cur_obj.get("umaban")
        if not umaban:
            return
        # ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®š
        cj = _norm_name(cur_obj.get("jockey", ""))
        pj = _norm_name(cur_obj.get("prev_jockey", ""))
        is_change = bool(pj and cj and pj != cj)
        cur_obj["is_change"] = is_change
        horses[str(umaban)] = cur_obj

    while i < len(lines):
        ln = lines[i]

        m = start_re.match(ln)
        if m:
            # æ–°ã—ã„é¦¬ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
            _finalize(cur)
            waku = m.group(1)
            umaban = m.group(2)

            cur = {
                "waku": waku,
                "umaban": umaban,
                "horse": "",
                "jockey": "",
                "trainer": "",
                "prev_jockey": "",
                "is_change": False,
            }

            # æ¬¡è¡Œï¼šé¦¬å
            if i + 1 < len(lines):
                cur["horse"] = lines[i + 1].strip()

            # èª¿æ•™å¸«ã¯ã€Œé¦¬ä¸» ç”Ÿç”£ç‰§å ´ èª¿æ•™å¸«ã€ãŒåŒä¸€è¡Œã«å‡ºã‚‹ã“ã¨ãŒå¤šã„
            # ä¾‹: "æ± è°·èª ä¸€   ä¸Šæ°´ç‰§å ´ ç¦ç”°çœŸ"
            # â†’ è¡Œæœ«ã®2ã€œ4æ–‡å­—ç¨‹åº¦ã®æ—¥æœ¬èªå§“åã‚’èª¿æ•™å¸«ã¨ã—ã¦æ‹¾ã†ï¼ˆæ‰€å±è¡Œã¯åˆ¥ã«ã‚ã‚‹ï¼‰
            # ãŸã ã—ç¢ºå®Ÿæ€§ã®ãŸã‚ã€ã€Œï¼ˆå¤§äº•ï¼‰ã€ç­‰ã®æ‰€å±è¡Œã®ç›´å‰ã‚ãŸã‚Šã‚‚æ¢ã™
            trainer = ""
            jockey = ""

            # ã–ã£ãã‚Šã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã®å‰åŠï¼ˆæ¬¡ã® start ã¾ã§ or 80è¡Œï¼‰ã‚’èµ°æŸ»ã—ã¦
            #  - ã€Œ(æ‰€å±) æ–¤é‡ã€ã®ç›´å¾Œã®è¡Œã‚’é¨æ‰‹ã¨ã¿ãªã™
            #  - ã€Œ(æ‰€å±) æ–¤é‡ã€ã®ç›´å‰ã®è¡Œã‚’å«ã‚€æ–‡è„ˆã§èª¿æ•™å¸«ã‚’æ‹¾ã†
            scan_end = min(len(lines), i + 120)
            k = i
            while k < scan_end:
                ln2 = lines[k]

                # é¨æ‰‹ï¼šæ–¤é‡è¡Œï¼ˆä¾‹: "(å¤§äº•) 54.0" / "(å¤§äº•)â–² 53.0"ï¼‰ã®æ¬¡è¡Œ
                if re.search(r"^\ï¼ˆ.*\ï¼‰\s*[â˜†â–²â–³â—‡]?\s*\d{1,2}\.\d$", ln2):
                    if k + 1 < len(lines):
                        jockey = lines[k + 1].strip()
                        cur["jockey"] = jockey

                # èª¿æ•™å¸«ï¼šèª¿æ•™å¸«ãŒå«ã¾ã‚Œã‚‹è¡Œï¼ˆé¦¬ä¸»/ç”Ÿç”£è€…/èª¿æ•™å¸«ãŒä¸¦ã¶è¡Œï¼‰ã‚’æ‹¾ã†
                # ä¾‹: "... ä¸Šæ°´ç‰§å ´ ç¦ç”°çœŸ"
                # â†’ æœ«å°¾ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èª¿æ•™å¸«ã¨ã—ã¦æ¡ç”¨
                if "ç‰§å ´" in ln2 or "ãƒ•ã‚¡ãƒ¼ãƒ " in ln2 or "æ ªå¼ä¼šç¤¾" in ln2 or "ï¼ˆæœ‰ï¼‰" in ln2 or "ï¼ˆæ ªï¼‰" in ln2:
                    parts = re.split(r"\s+", ln2)
                    if len(parts) >= 2:
                        cand = parts[-1].strip()
                        # æ‰€å±æ‹¬å¼§ãŒæ··ã˜ã‚‹ã‚±ãƒ¼ã‚¹ã¯é™¤å¤–
                        if "ï¼ˆ" not in cand and "ï¼‰" not in cand and len(cand) <= 6:
                            trainer = cand
                            cur["trainer"] = trainer

                # å‰èµ°é¨æ‰‹ï¼šæœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã ã‘æ¡ç”¨
                if not cur["prev_jockey"]:
                    pm = prev_jockey_re.match(ln2)
                    if pm:
                        cur["prev_jockey"] = pm.group(1).strip()

                # æ¬¡ã®é¦¬ãƒ–ãƒ­ãƒƒã‚¯å§‹ã¾ã‚Šã§æ‰“ã¡åˆ‡ã‚Š
                if k > i and start_re.match(ln2):
                    break

                k += 1

            # èª¿æ•™å¸«ãŒã¾ã ç©ºãªã‚‰ã€æ‰€å±è¡Œã®ã€Œä¸€ã¤å‰ã€ã‚’èª¿æ•™å¸«å€™è£œã¨ã—ã¦æ‹¾ã†ä¿é™º
            if not cur["trainer"]:
                # æ‰€å±è¡Œ "(å¤§äº•) 54.0" ã‚’è¦‹ã¤ã‘ãŸã‚‰ã€ãã®2ã¤å‰ãã‚‰ã„ã«èª¿æ•™å¸«ãŒã„ã‚‹ã“ã¨ãŒå¤šã„
                for kk in range(i, min(len(lines), i + 80)):
                    if re.search(r"^\ï¼ˆ.*\ï¼‰\s*[â˜†â–²â–³â—‡]?\s*\d{1,2}\.\d$", lines[kk]):
                        # 2ã¤å‰ã®è¡Œæœ«
                        if kk - 2 >= 0:
                            ln_tr = lines[kk - 2]
                            parts = re.split(r"\s+", ln_tr)
                            if parts:
                                cand = parts[-1].strip()
                                if "ï¼ˆ" not in cand and "ï¼‰" not in cand and len(cand) <= 6:
                                    cur["trainer"] = cand
                        break

            i += 1
            continue

        # ãƒ–ãƒ­ãƒƒã‚¯ä¸­ã®å‰èµ°é¨æ‰‹ï¼ˆæœ€åˆã®1å›ã ã‘ï¼‰
        if cur and not cur["prev_jockey"]:
            pm = prev_jockey_re.match(ln)
            if pm:
                cur["prev_jockey"] = pm.group(1).strip()

        i += 1

    _finalize(cur)
    return header, horses, url

# ==================================================
# Difyï¼ˆstreamingï¼‰
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot",
    }
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300,
        )

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue

            raw = decoded.replace("data: ", "").strip()
            if not raw:
                continue

            try:
                data = json.loads(raw)
            except:
                continue

            if "answer" in data and isinstance(data["answer"], str):
                yield data["answer"]

            if data.get("event") == "workflow_finished":
                out = data.get("data", {}).get("outputs", {})
                texts = [v for v in out.values() if isinstance(v, str)]
                if texts:
                    yield "".join(texts)

    except Exception as e:
        yield f"âš ï¸ API Error: {str(e)}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³ï¼šå…¨ãƒ¬ãƒ¼ã‚¹å®Ÿè¡Œ
# ==================================================
def run_all_races(year: str, month: str, day: str, place_code: str, target_races: set[int] | None):
    """
    place_codeï¼šç«¶é¦¬ãƒ–ãƒƒã‚¯å´ï¼ˆ10å¤§äº•/11å·å´/12èˆ¹æ©‹/13æµ¦å’Œï¼‰
    é¨æ‰‹ãƒ»èª¿æ•™å¸«ã¯ keiba.go.jp ã® babaCode ã‚’ä½¿ã†
    """
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(place_code, "åœ°æ–¹")

    # keiba.go.jp å´ã® babaCode
    baba_map = {"10": "20", "11": "21", "12": "19", "13": "18"}
    baba_code = baba_map.get(place_code)
    if not baba_code:
        st.error("babaCode mapping ãŒæœªå®šç¾©ã§ã™ã€‚place_code ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    driver = build_driver()
    wait = WebDriverWait(driver, 12)

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰")
        login_keibabook(driver, wait)

        race_ids = fetch_race_ids_from_schedule(driver, year, month, day, place_code)
        if not race_ids:
            return

        for i, race_id in enumerate(race_ids):
            race_num = i + 1
            if target_races is not None and race_num not in target_races:
                continue

            race_num_str = f"{race_num:02}"

            st.markdown(f"## {place_name} {race_num}R")
            st.caption(f"race_id(keibabook): {race_id}")

            status_area = st.empty()
            result_area = st.empty()

            try:
                status_area.info("ğŸ“¡ ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")

                # --------------------------
                # 0) keiba.go.jp å‡ºé¦¬è¡¨ï¼ˆé¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒ»å‰èµ°é¨æ‰‹ï¼‰
                # --------------------------
                header, keibago_dict, keibago_url = fetch_keibago_debatable_small(
                    year=str(year),
                    month=str(month),
                    day=str(day),
                    race_no=race_num,
                    baba_code=str(baba_code),
                )
                st.caption(f"keiba.go.jp: {keibago_url}")
                st.caption(f"keiba.go.jp header: {header}")

                if not keibago_dict:
                    st.warning("âš ï¸ keiba.go.jp ã‹ã‚‰å‡ºé¦¬è¡¨ãŒå–ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆç¶šè¡Œã—ã¾ã™ãŒé¨æ‰‹/èª¿æ•™å¸«ãŒä¸æ˜ã«ãªã‚Šã¾ã™ï¼‰")

                # --------------------------
                # 1) è«‡è©±ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰
                # --------------------------
                driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}")
                try:
                    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "danwa")))
                except:
                    pass

                html_danwa = driver.page_source
                race_meta = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)

                # --------------------------
                # 2) èª¿æ•™ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰
                # --------------------------
                driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{race_id}")
                try:
                    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "cyokyo")))
                except:
                    pass

                cyokyo_dict = parse_cyokyo(driver.page_source)

                # --------------------------
                # çµ±åˆï¼ˆé¦¬ç•ªã§æƒãˆã‚‹ï¼‰
                # --------------------------
                all_uma = sorted(
                    set(danwa_dict.keys()) | set(cyokyo_dict.keys()) | set(keibago_dict.keys()),
                    key=lambda x: int(x) if str(x).isdigit() else 999,
                )

                merged_text = []
                for uma in all_uma:
                    kg = keibago_dict.get(uma, {})
                    horse = kg.get("horse", "")
                    jockey = kg.get("jockey", "ä¸æ˜")
                    trainer = kg.get("trainer", "ä¸æ˜")
                    prev_jockey = kg.get("prev_jockey", "")
                    is_change = kg.get("is_change", False)

                    alert = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if is_change else ""
                    if prev_jockey:
                        alert += f"ï¼ˆå‰èµ°:{prev_jockey}ï¼‰"

                    d = danwa_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                    c = cyokyo_dict.get(uma, "ï¼ˆãªã—ï¼‰")

                    if jockey == "ä¸æ˜":
                        print(f"Warning: keiba.go.jp jockey not found for umaban={uma} race_num={race_num}")

                    merged_text.append(
                        f"â–¼[é¦¬ç•ª{uma}] é¦¬å:{horse} é¨æ‰‹:{jockey} {alert} èª¿æ•™å¸«:{trainer}\n"
                        f"è«‡è©±: {d}\n"
                        f"èª¿æ•™: {c}"
                    )

                if not merged_text:
                    status_area.warning("ãƒ‡ãƒ¼ã‚¿ãªã—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    st.divider()
                    continue

                prompt = (
                    f"ãƒ¬ãƒ¼ã‚¹å: {race_meta.get('race_name','')}\n"
                    f"æ¡ä»¶: {race_meta.get('cond','')}\n\n"
                    "ä»¥ä¸‹ã®å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆé¦¬åã€é¨æ‰‹ã€ä¹—ã‚Šæ›¿ã‚ã‚Šã€èª¿æ•™å¸«ã€è«‡è©±ã€èª¿æ•™ï¼‰ã§ã™ã€‚\n"
                    + "\n".join(merged_text)
                )

                status_area.info("ğŸ¤– AIåˆ†æä¸­...")
                full_ans = ""
                for chunk in stream_dify_workflow(prompt):
                    full_ans += chunk
                    result_area.markdown(full_ans + "â–Œ")

                result_area.markdown(full_ans)
                status_area.success("âœ… å®Œäº†")

                save_history(year, place_code, place_name, month, day, race_num_str, race_id, full_ans)

            except Exception as e:
                status_area.error(f"Error: {e}")

            st.divider()

    finally:
        driver.quit()
