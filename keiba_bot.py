import time
import json
import re
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã€‘Secretsèª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¤‰æ•°
YEAR = "2025"
PLACE_CODE = "11"  # 10:å¤§äº•, 11:å·å´, 12:èˆ¹æ©‹, 13:æµ¦å’Œ
MONTH = "12"
DAY = "29"

def set_race_params(year, place_code, month, day):
    global YEAR, PLACE_CODE, MONTH, DAY
    YEAR = str(year)
    PLACE_CODE = str(place_code).zfill(2)
    MONTH = str(month).zfill(2)
    DAY = str(day).zfill(2)

# ==================================================
# Supabase & Helper
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if not supabase: return
    data = {
        "year": str(year),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "month": str(month),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }
    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)

# ==================================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°ç¾¤
# ==================================================

# 1. ãƒ¬ãƒ¼ã‚¹IDä¸€è¦§ã‚’å–å¾—
def fetch_race_ids_from_schedule(driver, year, month, day, target_place_code):
    date_str = f"{year}{month}{day}"
    url = f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10"
    
    st.info(f"ğŸ“… æ—¥ç¨‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ä¸­... ({url})")
    driver.get(url)
    
    # æ—¥ç¨‹ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå‡ºã‚‹ã¾ã§å°‘ã—å¾…ã¤
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "a")))
    except:
        pass
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    race_ids = []
    seen = set()
    
    for a in soup.find_all("a", href=True):
        href = a['href']
        match = re.search(r'(\d{16})', href)
        if match:
            rid = match.group(1)
            # IDã®5-6æ¡ç›®ãŒç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
            if rid[6:8] == target_place_code:
                if rid not in seen:
                    race_ids.append(rid)
                    seen.add(rid)
    
    race_ids.sort()
    
    if not race_ids:
        st.warning(f"âš ï¸ æŒ‡å®šã—ãŸç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰({target_place_code})ã®ãƒ¬ãƒ¼ã‚¹IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.success(f"âœ… {len(race_ids)} ä»¶ã®ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        
    return race_ids

# 2. é¨æ‰‹æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼šHTMLæ§‹é€ ã«å®Œå…¨å¯¾å¿œï¼‰
def parse_syutuba_jockey(html: str):
    soup = BeautifulSoup(html, "html.parser")
    jockey_info = {}
    
    # CSSã‚»ãƒ¬ã‚¯ã‚¿ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š (.syutuba_sp ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ table)
    table = soup.select_one("table.syutuba_sp")
    if not table:
        # ãƒ‡ãƒãƒƒã‚°: ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        return {}

    # è¡Œã‚’å–å¾— (theadã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã€tbodyãŒã‚ã‚Œã°ãã“ã‹ã‚‰å–ã‚‹)
    rows = table.select("tbody tr") if table.find("tbody") else table.find_all("tr")
    
    for row in rows:
        tds = row.find_all("td")
        if not tds:
            continue
            
        # 1åˆ—ç›®ãŒé¦¬ç•ªï¼ˆæ•°å­—ï¼‰ã§ã‚ã‚‹ã‹ç¢ºèª
        umaban_text = tds[0].get_text(strip=True)
        if not umaban_text.isdigit():
            continue
        
        umaban = umaban_text

        # é¨æ‰‹æ¬„ã¯ <p class="kisyu">
        # CSSã‚»ãƒ¬ã‚¯ã‚¿ ".kisyu" ã§æ¢ã™ã®ãŒç¢ºå®Ÿ
        kisyu_p = row.select_one(".kisyu")
        
        if kisyu_p:
            name = ""
            is_change = False
            
            # å„ªå…ˆ1: <a>ã‚¿ã‚°ã‚’æ¢ã™ (HTMLã§ã¯ã“ã“ã«å…¥ã£ã¦ã„ã‚‹)
            # ä¾‹: <a href="...">æ¡‘æ‘çœŸ</a>
            anchor = kisyu_p.find("a")
            
            if anchor:
                name = anchor.get_text(strip=True)
                # ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®š: <a>ã®ä¸­ã«<strong>ãŒã‚ã‚‹ã‹
                if anchor.find("strong"):
                    is_change = True
            
            # å„ªå…ˆ2: <a>ãŒãªã„å ´åˆ (ãƒªãƒ³ã‚¯åˆ‡ã‚Œé¨æ‰‹ãªã©)
            else:
                full_text = kisyu_p.get_text(" ", strip=True)
                if kisyu_p.find("strong"):
                    is_change = True
                    name = kisyu_p.find("strong").get_text(strip=True)
                else:
                    # åˆ†å‰²ã—ã¦è§£æ: "ç‰¡2 é¨æ‰‹å 55"
                    parts = full_text.split()
                    for part in parts:
                        # æ•°å­—ã‚’å«ã¾ãªã„ã€ã‹ã¤æ€§åˆ¥è¨˜å·ã§ãªã„ã‚‚ã®ã‚’å€™è£œã¨ã™ã‚‹
                        if not any(char.isdigit() for char in part) and part not in ["ç‰¡", "ç‰", "ã‚»ãƒ³", "â–²", "â–³", "â˜†", "â—‡"]:
                            name = part
                            break
                    if not name and len(parts) >= 2:
                        name = parts[1]

            if name:
                jockey_info[umaban] = {"name": name, "is_change": is_change}
            
    return jockey_info

# 3. ãã®ä»–æƒ…å ±ã®ãƒ‘ãƒ¼ã‚¹
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle: return {}
    
    racemei = racetitle.find("div", class_="racemei")
    # pã‚¿ã‚°ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã€2ç•ªç›®ãŒãƒ¬ãƒ¼ã‚¹åã®ã“ã¨ãŒå¤šã„
    p_tags = racemei.find_all("p") if racemei else []
    race_name = p_tags[1].get_text(strip=True) if len(p_tags) >= 2 else (p_tags[0].get_text(strip=True) if p_tags else "")
    
    sub = racetitle.find("div", class_="racetitle_sub")
    sub_p = sub.find_all("p") if sub else []
    cond = sub_p[1].get_text(" ", strip=True) if len(sub_p) >= 2 else ""
    
    return {"race_name": race_name, "cond": cond}

def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    danwa_dict = {}
    table = soup.find("table", class_="danwa")
    
    if table and table.tbody:
        current_uma = None
        for row in table.tbody.find_all("tr"):
            uma_td = row.find("td", class_="umaban")
            if uma_td:
                current_uma = uma_td.get_text(strip=True)
                continue
            txt_td = row.find("td", class_="danwa")
            if txt_td and current_uma:
                danwa_dict[current_uma] = txt_td.get_text(strip=True)
                current_uma = None
    return danwa_dict

def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    tables = soup.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows: continue
        
        h_row = rows[0]
        uma_td = h_row.find("td", class_="umaban")
        name_td = h_row.find("td", class_="kbamei")
        if not uma_td or not name_td: continue
        
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        
        tanpyo_elem = h_row.find("td", class_="tanpyo")
        tanpyo = tanpyo_elem.get_text(strip=True) if tanpyo_elem else ""
        detail = rows[1].get_text(" ", strip=True) if len(rows) > 1 else ""
        
        cyokyo_dict[umaban] = f"ã€é¦¬åã€‘{bamei} ã€çŸ­è©•ã€‘{tanpyo} ã€è©³ç´°ã€‘{detail}"
    return cyokyo_dict

# ==================================================
# Dify é€£æº
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
        return
    
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=300)
        for line in res.iter_lines():
            if line:
                decoded = line.decode('utf-8')
                if decoded.startswith("data:"):
                    try:
                        data = json.loads(decoded.replace("data: ", ""))
                        if data.get("event") == "workflow_finished":
                            out = data.get("data", {}).get("outputs", {})
                            yield "".join([v for v in out.values() if isinstance(v, str)])
                        elif "answer" in data:
                            yield data.get("answer", "")
                    except: pass
    except Exception as e:
        yield f"âš ï¸ API Error: {str(e)}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
def run_all_races(target_races=None):
    place_names = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}
    place_name = place_names.get(PLACE_CODE, "åœ°æ–¹")

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    # â˜…å¤‰æ›´: æ˜ç¤ºçš„ãªWaitã‚’è¨­å®š
    wait = WebDriverWait(driver, 10)

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")
        
        # ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ å¾…æ©Ÿ
        wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(1) # ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†å®Œäº†å¾…ã¡

        race_ids = fetch_race_ids_from_schedule(driver, YEAR, MONTH, DAY, PLACE_CODE)
        
        if not race_ids:
            return

        for i, race_id in enumerate(race_ids):
            race_num = i + 1
            if target_races is not None and race_num not in target_races:
                continue

            race_num_str = f"{race_num:02}"
            st.markdown(f"### {place_name} {race_num}R (ID: {race_id})")
            status_area = st.empty()
            result_area = st.empty()
            
            try:
                status_area.info("ğŸ“¡ ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                
                # 1. è«‡è©±ãƒšãƒ¼ã‚¸
                driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}")
                try:
                    # è«‡è©±ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå‡ºã‚‹ã¾ã§å¾…ã¤
                    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "danwa")))
                except:
                    st.warning("è«‡è©±ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™ã€‚")
                
                html_danwa = driver.page_source
                race_meta = parse_race_info(html_danwa)
                danwa_dict = parse_danwa_comments(html_danwa)
                
                # 2. å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ï¼ˆâ˜…ã“ã“ãŒæœ€é‡è¦ä¿®æ­£ç®‡æ‰€ï¼‰
                driver.get(f"https://s.keibabook.co.jp/chihou/syutuba/{race_id}")
                try:
                    # 'syutuba_sp' ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤
                    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "syutuba_sp")))
                    time.sleep(1) # æç”»å®Œäº†ã®ãƒãƒƒãƒ•ã‚¡
                except:
                    st.warning("å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")

                jockey_dict = parse_syutuba_jockey(driver.page_source)
                
                # 3. èª¿æ•™ãƒšãƒ¼ã‚¸
                driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{race_id}")
                try:
                    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "cyokyo")))
                except:
                    pass
                cyokyo_dict = parse_cyokyo(driver.page_source)
                
                # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
                merged_text = []
                all_uma = sorted(list(set(list(danwa_dict.keys()) + list(cyokyo_dict.keys()) + list(jockey_dict.keys()))), 
                                 key=lambda x: int(x) if x.isdigit() else 99)
                
                for uma in all_uma:
                    j = jockey_dict.get(uma, {"name": "ä¸æ˜", "is_change": False})
                    d = danwa_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                    c = cyokyo_dict.get(uma, "ï¼ˆãªã—ï¼‰")
                    
                    alert = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if j["is_change"] else ""
                    # é¨æ‰‹åãŒã€Œä¸æ˜ã€ãªã‚‰è­¦å‘Šãƒ­ã‚°ã‚’å‡ºã™(é–‹ç™ºæ™‚ç¢ºèªç”¨)
                    if j["name"] == "ä¸æ˜":
                        print(f"Warning: Jockey not found for umaban {uma}")

                    merged_text.append(f"â–¼[é¦¬ç•ª{uma}] {j['name']} {alert}\n è«‡è©±: {d}\n èª¿æ•™: {c}")

                if not merged_text:
                    status_area.warning("ãƒ‡ãƒ¼ã‚¿ãªã—ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                prompt = (
                    f"ãƒ¬ãƒ¼ã‚¹å: {race_meta.get('race_name','')}\n"
                    f"æ¡ä»¶: {race_meta.get('cond','')}\n\n"
                    "ä»¥ä¸‹ã®å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆé¨æ‰‹ã€èª¿æ•™å¸«ã€è«‡è©±ã€èª¿æ•™ï¼‰ã§ã™ã€‚\n"
                    + "\n".join(merged_text)
                )
                
                status_area.info("ğŸ¤– AIåˆ†æä¸­...")
                full_ans = ""
                for chunk in stream_dify_workflow(prompt):
                    full_ans += chunk
                    result_area.markdown(full_ans + "â–Œ")
                
                result_area.markdown(full_ans)
                status_area.success("å®Œäº†")
                save_history(YEAR, PLACE_CODE, place_name, MONTH, DAY, race_num_str, race_id, full_ans)

            except Exception as e:
                status_area.error(f"Error: {e}")
                
            st.write("---")

    finally:
        driver.quit()
