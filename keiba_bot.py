import time
import json
import re
import requests
import streamlit as st
from datetime import datetime
import pytz
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# 1. è¨­å®šãƒ»å®šæ•°ãƒ»Secretsèª­ã¿è¾¼ã¿
# ==================================================

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼
def check_password():
    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False
    if st.session_state["password_correct"]: return True
    
    st.title("ğŸ”’ ãƒ­ã‚°ã‚¤ãƒ³")
    ADMIN_PASS = st.secrets.get("ADMIN_PASSWORD", "admin123")
    user_input = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
        if user_input == ADMIN_PASS:
            st.session_state["password_correct"] = True
            st.rerun()
        else: st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")
    return False

if not check_password(): st.stop()

# Secrets
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# å ´æ‰€åãƒãƒƒãƒ—
PLACE_NAMES = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}

# ==================================================
# 2. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (Supabase, Driver)
# ==================================================

@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY: return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if not supabase: return
    data = {
        "year": str(year),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "month": str(month),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }
    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        st.error(f"Supabase save error: {e}")

def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=options)

# ==================================================
# 3. ç«¶é¦¬ãƒ–ãƒƒã‚¯ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
# ==================================================

def login_keibabook(driver):
    if not KEIBA_ID or not KEIBA_PASS:
        st.warning("âš ï¸ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®ID/PASSãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
    try:
        driver.get("https://s.keibabook.co.jp/login/login")
        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(1)
        return True
    except Exception as e:
        st.error(f"ç«¶é¦¬ãƒ–ãƒƒã‚¯ ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def fetch_race_ids_from_schedule(driver, year, month, day, target_place_code):
    date_str = f"{year}{month}{day}"
    url = f"https://s.keibabook.co.jp/chihou/nittei/{date_str}10" 
    
    st.info(f"ğŸ“… æ—¥ç¨‹å–å¾—ä¸­: {url}")
    driver.get(url)
    time.sleep(1)
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    race_ids = []
    seen = set()
    
    for a in soup.find_all("a", href=True):
        href = a['href']
        # ä¾‹: /chihou/syutuba/2025191003011226
        match = re.search(r'(\d{16})', href)
        if match:
            rid = match.group(1)
            # IDã®6-7æ–‡å­—ç›®(å ´æ‰€ã‚³ãƒ¼ãƒ‰)ç¢ºèª
            if rid[6:8] == target_place_code:
                if rid not in seen:
                    race_ids.append(rid)
                    seen.add(rid)
    race_ids.sort()
    return race_ids

def parse_race_info(html: str):
    """ãƒ¬ãƒ¼ã‚¹åãƒ»æ¡ä»¶ãªã©ã‚’å–å¾—"""
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle: return {}
    
    racemei = racetitle.find("div", class_="racemei")
    race_name = racemei.find_all("p")[1].get_text(strip=True) if racemei and len(racemei.find_all("p")) >= 2 else ""
    
    sub = racetitle.find("div", class_="racetitle_sub")
    cond = sub.find_all("p")[1].get_text(" ", strip=True) if sub and len(sub.find_all("p")) >= 2 else ""
    return {"race_name": race_name, "cond": cond}

def parse_danwa_comments(html: str):
    """
    è«‡è©±ï¼ˆå©èˆã‚³ãƒ¡ãƒ³ãƒˆï¼‰ã‚’å–å¾—ã€‚
    â€»ã“ã“ã«ã€Œã€‡ã€‡å¸«ã€ãªã©ã®èª¿æ•™å¸«åãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒå¤šã„
    """
    soup = BeautifulSoup(html, "html.parser")
    danwa_dict = {}
    table = soup.find("table", class_="danwa")
    if table and table.tbody:
        current_uma = None
        for row in table.tbody.find_all("tr"):
            uma_td = row.find("td", class_="umaban")
            if uma_td:
                current_uma = uma_td.get_text(strip=True)
                continue
            txt_td = row.find("td", class_="danwa")
            if txt_td and current_uma:
                danwa_dict[current_uma] = txt_td.get_text(strip=True)
                current_uma = None
    return danwa_dict

def parse_syutuba_jockey(html: str):
    """
    å‡ºé¦¬è¡¨ã‹ã‚‰ã€Œé¨æ‰‹ã€ã¨ã€Œä¹—ã‚Šæ›¿ã‚ã‚Šã€ã‚’å–å¾—
    """
    soup = BeautifulSoup(html, "html.parser")
    info = {}
    
    # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ç‰ˆã®å„é¦¬ãƒ–ãƒ­ãƒƒã‚¯
    sections = soup.find_all("div", class_="section")
    
    for sec in sections:
        umaban_div = sec.find("div", class_="umaban")
        if not umaban_div: continue
        umaban = umaban_div.get_text(strip=True)
        
        kisyu_p = sec.find("p", class_="kisyu")
        jockey_name = "ä¸æ˜"
        is_change = False
        
        if kisyu_p:
            jockey_a = kisyu_p.find("a")
            if jockey_a:
                jockey_name = jockey_a.get_text(strip=True)
                # ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®š(strong/b)
                if jockey_a.find("strong") or jockey_a.find("b"):
                    is_change = True
            else:
                jockey_name = kisyu_p.get_text(strip=True)
            
            # è¦ªè¦ç´ ãƒ¬ãƒ™ãƒ«ã§ã®å¼·èª¿ãƒã‚§ãƒƒã‚¯
            if kisyu_p.find("strong") or kisyu_p.find("b") or "red" in kisyu_p.get("class", []):
                is_change = True

        info[umaban] = {
            "jockey": jockey_name,
            "is_change": is_change
        }
            
    return info

def parse_cyokyo(html: str):
    """èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    tables = soup.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        if not rows: continue
        
        h_row = rows[0]
        uma_td = h_row.find("td", class_="umaban")
        name_td = h_row.find("td", class_="kbamei")
        if not uma_td or not name_td: continue
        
        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)
        tanpyo = h_row.find("td", class_="tanpyo").get_text(strip=True) if h_row.find("td", class_="tanpyo") else ""
        detail = rows[1].get_text(" ", strip=True) if len(rows) > 1 else ""
        
        cyokyo_dict[umaban] = f"ã€é¦¬åã€‘{bamei} ã€çŸ­è©•ã€‘{tanpyo} ã€è©³ç´°ã€‘{detail}"
    return cyokyo_dict

# ==================================================
# 4. Dify APIé€£æº
# ==================================================

def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEYæœªè¨­å®š"
        return
    
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=300)
        for line in res.iter_lines():
            if line:
                decoded = line.decode('utf-8')
                if decoded.startswith("data:"):
                    try:
                        data = json.loads(decoded.replace("data: ", ""))
                        if "answer" in data:
                            yield data.get("answer", "")
                    except: pass
    except Exception as e:
        yield f"âš ï¸ API Error: {str(e)}"

# ==================================================
# 5. ãƒ¡ã‚¤ãƒ³ç”»é¢ãƒ»å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
# ==================================================

st.title("ğŸ‡ ç«¶é¦¬ãƒ–ãƒƒã‚¯å°‚ç”¨ AIåˆ†æBot")
jst = pytz.timezone('Asia/Tokyo')
now = datetime.now(jst)

# è¨­å®šUI
with st.container():
    c1, c2 = st.columns(2)
    with c1: target_date = st.date_input("åˆ†ææ—¥", now)
    with c2: 
        PLACE_CODE = st.selectbox("é–‹å‚¬å ´æ‰€", ["10", "11", "12", "13"], 
                                  format_func=lambda x: f"{x}: {PLACE_NAMES.get(x)}")
    
    st.write("### ğŸ ãƒ¬ãƒ¼ã‚¹é¸æŠ")
    all_races = st.checkbox("å…¨ãƒ¬ãƒ¼ã‚¹ã‚’ä¸€æ‹¬åˆ†æã™ã‚‹", value=True)
    target_races = []
    if not all_races:
        cols = st.columns(6)
        for i in range(1, 13):
            with cols[(i-1)//2]:
                if st.checkbox(f"{i}R", key=f"r{i}"): target_races.append(i)
    else:
        target_races = list(range(1, 13))

# å®Ÿè¡Œãƒœã‚¿ãƒ³
if st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary"):
    date_str = target_date.strftime("%Y%m%d")
    year_str = target_date.strftime("%Y")
    month_str = target_date.strftime("%m")
    day_str = target_date.strftime("%d")
    place_name = PLACE_NAMES.get(PLACE_CODE, "ä¸æ˜")

    driver = get_driver()
    
    try:
        st.info("ğŸ”‘ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã¸ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        login_keibabook(driver)
        
        st.info("ğŸ“¡ ãƒ¬ãƒ¼ã‚¹IDã‚’å–å¾—ä¸­...")
        race_ids = fetch_race_ids_from_schedule(driver, year_str, month_str, day_str, PLACE_CODE)
        
        if not race_ids:
            st.error("ãƒ¬ãƒ¼ã‚¹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¼‘å‚¬æ—¥ã‹å ´æ‰€ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            for race_id in race_ids:
                race_num = int(race_id[10:12])
                if target_races and race_num not in target_races:
                    continue
                
                st.markdown(f"### {place_name} {race_num}R")
                status_area = st.empty()
                result_area = st.empty()
                
                try:
                    status_area.info("ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")
                    
                    # 1. è«‡è©±ãƒ»ãƒ¬ãƒ¼ã‚¹æƒ…å ±
                    driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{race_id}")
                    html_danwa = driver.page_source
                    race_meta = parse_race_info(html_danwa)
                    danwa_dict = parse_danwa_comments(html_danwa)
                    
                    # 2. å‡ºé¦¬è¡¨ (é¨æ‰‹ãƒ»ä¹—ã‚Šæ›¿ã‚ã‚Š)
                    driver.get(f"https://s.keibabook.co.jp/chihou/syutuba/{race_id}")
                    syutuba_info = parse_syutuba_jockey(driver.page_source)
                    
                    # 3. èª¿æ•™
                    driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{race_id}")
                    cyokyo_dict = parse_cyokyo(driver.page_source)
                    
                    # 4. ãƒ‡ãƒ¼ã‚¿çµåˆ
                    merged_text = []
                    all_uma = sorted(list(set(list(syutuba_info.keys()) + list(danwa_dict.keys()))), 
                                     key=lambda x: int(x) if x.isdigit() else 999)
                    
                    for uma in all_uma:
                        # é¨æ‰‹æƒ…å ±
                        s_info = syutuba_info.get(uma, {"jockey": "ä¸æ˜", "is_change": False})
                        jockey_name = s_info["jockey"]
                        is_change = s_info["is_change"]
                        alert = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if is_change else ""
                        
                        # è«‡è©± (ã“ã“ã«èª¿æ•™å¸«åã‚‚å«ã¾ã‚Œã‚‹æƒ³å®š)
                        danwa_txt = danwa_dict.get(uma, "ï¼ˆè«‡è©±ãªã—ï¼‰")
                        
                        # èª¿æ•™
                        cyokyo_txt = cyokyo_dict.get(uma, "ï¼ˆèª¿æ•™æƒ…å ±ãªã—ï¼‰")
                        
                        line = (
                            f"â–¼[é¦¬ç•ª{uma}]\n"
                            f"  é¨æ‰‹: {jockey_name} {alert}\n"
                            f"  å©èˆã‚³ãƒ¡ãƒ³ãƒˆ: {danwa_txt}\n"
                            f"  èª¿æ•™: {cyokyo_txt}"
                        )
                        merged_text.append(line)

                    if not merged_text:
                        status_area.warning("ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—")
                        continue

                    # 5. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
                    prompt = (
                        f"ãƒ¬ãƒ¼ã‚¹å: {race_meta.get('race_name','')}\n"
                        f"æ¡ä»¶: {race_meta.get('cond','')}\n\n"
                        "ä»¥ä¸‹ã®å„é¦¬ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆé¨æ‰‹ã€å©èˆã‚³ãƒ¡ãƒ³ãƒˆã€èª¿æ•™ï¼‰ã§ã™ã€‚\n"
                        + "\n".join(merged_text)
                    )
                    
                    # 6. AIåˆ†æ
                    status_area.info("ğŸ¤– AIåˆ†æã‚’å®Ÿè¡Œä¸­...")
                    full_ans = ""
                    for chunk in stream_dify_workflow(prompt):
                        full_ans += chunk
                        result_area.markdown(full_ans + "â–Œ")
                    
                    result_area.markdown(full_ans)
                    status_area.success("åˆ†æå®Œäº†")
                    
                    # å±¥æ­´ä¿å­˜
                    save_history(year_str, PLACE_CODE, place_name, month_str, day_str, f"{race_num:02}", race_id, full_ans)
                    
                except Exception as e:
                    status_area.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                
                st.divider()

    finally:
        driver.quit()
