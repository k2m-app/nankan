import time
import json
import re
import requests
import streamlit as st
from datetime import datetime
import pytz
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# 1. è¨­å®šãƒ»Secrets
# ==================================================

def check_password():
    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False
    if st.session_state["password_correct"]: return True
    
    st.title("ğŸ”’ ãƒ­ã‚°ã‚¤ãƒ³")
    ADMIN_PASS = st.secrets.get("ADMIN_PASSWORD", "admin123")
    val = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if st.button("Login"):
        if val == ADMIN_PASS:
            st.session_state["password_correct"] = True
            st.rerun()
        else: st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")
    return False

if not check_password(): st.stop()

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

PLACE_NAMES = {"10": "å¤§äº•", "11": "å·å´", "12": "èˆ¹æ©‹", "13": "æµ¦å’Œ"}

# ==================================================
# 2. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ==================================================

@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY: return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(year, place_code, place_name, month, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if not supabase: return
    data = {
        "year": str(year), "place_code": str(place_code), "place_name": place_name,
        "day": str(day), "month": str(month), "race_num": race_num_str,
        "race_id": race_id, "output_text": ai_answer
    }
    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        st.error(f"Supabase error: {e}")

def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,1080")
    # Botå¯¾ç­–å›é¿
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=options)

def login_keibabook(driver):
    if not KEIBA_ID or not KEIBA_PASS:
        st.warning("âš ï¸ ID/PASSæœªè¨­å®š")
        return False
    try:
        driver.get("https://s.keibabook.co.jp/login/login")
        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        driver.find_element(By.CSS_SELECTOR, "input[type='password']").send_keys(KEIBA_PASS)
        driver.find_element(By.CSS_SELECTOR, "input[type='submit']").click()
        time.sleep(1)
        return True
    except: return False

def fetch_race_ids(driver, year, month, day, p_code):
    url = f"https://s.keibabook.co.jp/chihou/nittei/{year}{month}{day}10"
    st.info(f"ğŸ“… æ—¥ç¨‹å–å¾—: {url}")
    driver.get(url)
    time.sleep(1)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    ids = []
    seen = set()
    for a in soup.find_all("a", href=True):
        m = re.search(r'(\d{16})', a['href'])
        if m:
            rid = m.group(1)
            if rid[6:8] == p_code and rid not in seen:
                ids.append(rid)
                seen.add(rid)
    return sorted(ids)

def get_race_meta(html):
    soup = BeautifulSoup(html, "html.parser")
    rt = soup.find("div", class_="racetitle")
    if not rt: return {}
    rm = rt.find("div", class_="racemei")
    rname = rm.find_all("p")[1].get_text(strip=True) if rm and len(rm.find_all("p")) > 1 else ""
    rs = rt.find("div", class_="racetitle_sub")
    cond = rs.find_all("p")[1].get_text(" ", strip=True) if rs and len(rs.find_all("p")) > 1 else ""
    return {"name": rname, "cond": cond}

# ==================================================
# 3. ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ (ç¢ºå®Ÿæ€§é‡è¦–)
# ==================================================

def parse_syutuba_page(html):
    """
    ã€å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã€‘ã‹ã‚‰ é¦¬ç•ªãƒ»é¦¬åãƒ»é¨æ‰‹ãƒ»ä¹—ã‚Šæ›¿ã‚ã‚Š ã‚’å–å¾—
    â€»ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼(syutuba_sp)ã¨ãƒ–ãƒ­ãƒƒã‚¯å½¢å¼(div.section)ã®ä¸¡å¯¾å¿œ
    """
    soup = BeautifulSoup(html, "html.parser")
    data = {}

    # --- ãƒ‘ã‚¿ãƒ¼ãƒ³A: ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ (syutuba_sp) ---
    table = soup.find("table", class_="syutuba_sp")
    if table:
        for row in table.find_all("tr"):
            # é¦¬ç•ªå–å¾— (class="umaban" ã¾ãŸã¯ 1åˆ—ç›®ã®æ•°å­—)
            umaban = None
            u_td = row.find("td", class_="umaban")
            if u_td:
                umaban = u_td.get_text(strip=True)
            else:
                tds = row.find_all("td")
                if tds and tds[0].get_text(strip=True).isdigit():
                    umaban = tds[0].get_text(strip=True)
            
            if not umaban: continue

            # é¦¬å
            b_td = row.find("td", class_="bamei")
            # ã‚¯ãƒ©ã‚¹ãŒãªã„å ´åˆã¯é©å½“ã«æ¢ã™ï¼ˆé€šå¸¸ã¯class="bamei"ãŒã‚ã‚‹ï¼‰
            horse_name = b_td.get_text(strip=True) if b_td else "ä¸æ˜"
            if horse_name == "ä¸æ˜":
                # ãƒªãƒ³ã‚¯å†…ã®é¦¬åã‚’æ¢ã™
                a_tag = row.find("a", href=re.compile("uma"))
                if a_tag: horse_name = a_tag.get_text(strip=True)

            # é¨æ‰‹
            k_p = row.find("p", class_="kisyu")
            jockey = "ä¸æ˜"
            is_change = False
            if k_p:
                jockey = k_p.get_text(strip=True)
                # ä¹—ã‚Šæ›¿ã‚ã‚Šåˆ¤å®š (strong, b, class="red")
                if k_p.find(["strong", "b"]) or "red" in k_p.get("class", []):
                    is_change = True

            data[umaban] = {"horse": horse_name, "jockey": jockey, "is_change": is_change}
            
    # --- ãƒ‘ã‚¿ãƒ¼ãƒ³B: ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã€divãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™ ---
    if not data:
        sections = soup.find_all("div", class_="section")
        for sec in sections:
            u_div = sec.find("div", class_="umaban")
            if not u_div: continue
            umaban = u_div.get_text(strip=True)
            
            # é¦¬å
            h_div = sec.find("div", class_="bamei")
            horse_name = h_div.get_text(strip=True) if h_div else "ä¸æ˜"
            
            # é¨æ‰‹
            k_p = sec.find("p", class_="kisyu")
            jockey = k_p.get_text(strip=True) if k_p else "ä¸æ˜"
            is_change = False
            if k_p and (k_p.find(["strong", "b"]) or "red" in k_p.get("class", [])):
                is_change = True
            
            data[umaban] = {"horse": horse_name, "jockey": jockey, "is_change": is_change}
            
    return data

def parse_danwa_page(html):
    """
    ã€è«‡è©±ãƒšãƒ¼ã‚¸ã€‘ã‹ã‚‰ å©èˆã®è©±(ã‚³ãƒ¡ãƒ³ãƒˆ) ã‚’å–å¾—
    â€»èª¿æ•™å¸«åã¯ã‚³ãƒ¡ãƒ³ãƒˆæ–‡è„ˆã‹ã‚‰æŠ½å‡º
    """
    soup = BeautifulSoup(html, "html.parser")
    data = {}
    table = soup.find("table", class_="danwa")
    if table and table.tbody:
        current_uma = None
        for row in table.tbody.find_all("tr"):
            # é¦¬ç•ªè¡Œ
            u_td = row.find("td", class_="umaban")
            if u_td:
                current_uma = u_td.get_text(strip=True)
                continue
            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œ
            txt_td = row.find("td", class_="danwa")
            if txt_td and current_uma:
                text = txt_td.get_text(strip=True)
                # èª¿æ•™å¸«åã®æŠ½å‡º (ä¾‹: "ã€‡ã€‡å¸«" ã‚’æ¢ã™)
                trainer = "ä¸æ˜"
                m = re.search(r'(\S+å¸«)', text)
                if m: trainer = m.group(1)
                
                data[current_uma] = {"comment": text, "trainer": trainer}
                current_uma = None
    return data

def parse_cyokyo_page(html):
    """
    ã€èª¿æ•™ãƒšãƒ¼ã‚¸ã€‘ã‹ã‚‰ èª¿æ•™ã‚¿ã‚¤ãƒ ãƒ»çŸ­è©• ã‚’å–å¾—
    """
    soup = BeautifulSoup(html, "html.parser")
    data = {}
    # è¤‡æ•°ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚‹å ´åˆã«å¯¾å¿œ
    tables = soup.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        # 2è¡Œ1ã‚»ãƒƒãƒˆ (1è¡Œç›®:é¦¬ç•ªãƒ»é¦¬åãƒ»çŸ­è©• / 2è¡Œç›®:è©³ç´°ã‚¿ã‚¤ãƒ )
        if len(rows) < 2: continue
        
        # 1è¡Œç›®è§£æ
        r1 = rows[0]
        u_td = r1.find("td", class_="umaban")
        if not u_td: continue
        umaban = u_td.get_text(strip=True)
        
        tanpyo_td = r1.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        
        # 2è¡Œç›®è§£æ (è©³ç´°)
        r2 = rows[1]
        detail = r2.get_text(" ", strip=True)
        
        data[umaban] = {"tanpyo": tanpyo, "time": detail}
        
    return data

# ==================================================
# 4. Dify API
# ==================================================

def stream_dify(text):
    if not DIFY_API_KEY:
        yield "âš ï¸ API Keyæœªè¨­å®š"
        return
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    payload = {"inputs": {"text": text}, "response_mode": "streaming", "user": "keiba-bot"}
    
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=120)
        for line in res.iter_lines():
            if line:
                d = line.decode('utf-8')
                if d.startswith("data:"):
                    try:
                        j = json.loads(d.replace("data: ", ""))
                        if "answer" in j: yield j["answer"]
                    except: pass
    except Exception as e:
        yield f"Error: {e}"

# ==================================================
# 5. ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
# ==================================================

st.title("ğŸ‡ ç«¶é¦¬ãƒ–ãƒƒã‚¯å®Œå…¨å–å¾—Bot")
jst = pytz.timezone('Asia/Tokyo')
now = datetime.now(jst)

with st.container():
    c1, c2 = st.columns(2)
    with c1: target_date = st.date_input("æ—¥ä»˜", now)
    with c2: PLACE_CODE = st.selectbox("å ´æ‰€", ["10","11","12","13"], format_func=lambda x: f"{x}:{PLACE_NAMES[x]}")
    
    all_races = st.checkbox("å…¨ãƒ¬ãƒ¼ã‚¹", value=True)
    target_races = []
    if not all_races:
        cols = st.columns(6)
        for i in range(1, 13):
            with cols[(i-1)//2]:
                if st.checkbox(f"{i}R", key=f"r{i}"): target_races.append(i)
    else: target_races = list(range(1,13))

if st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary"):
    ymd = target_date.strftime("%Y%m%d")
    pname = PLACE_NAMES[PLACE_CODE]
    driver = get_driver()
    
    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        if not login_keibabook(driver):
            st.stop()
            
        st.info("ğŸ“¡ ãƒ¬ãƒ¼ã‚¹IDå–å¾—ä¸­...")
        rids = fetch_race_ids(driver, target_date.strftime("%Y"), target_date.strftime("%m"), target_date.strftime("%d"), PLACE_CODE)
        
        if not rids:
            st.error("ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            for rid in rids:
                rnum = int(rid[10:12])
                if target_races and rnum not in target_races: continue
                
                st.markdown(f"### {pname} {rnum}R")
                status = st.empty()
                res_area = st.empty()
                
                try:
                    status.info("ğŸ“š ãƒ‡ãƒ¼ã‚¿åé›†ä¸­ (å‡ºé¦¬è¡¨/è«‡è©±/èª¿æ•™)...")
                    
                    # 1. å‡ºé¦¬è¡¨ (é¨æ‰‹ãƒ»ä¹—ã‚Šæ›¿ã‚ã‚Šãƒ»é¦¬å)
                    driver.get(f"https://s.keibabook.co.jp/chihou/syutuba/{rid}")
                    syutuba_data = parse_syutuba_page(driver.page_source)
                    
                    # 2. è«‡è©± (ã‚³ãƒ¡ãƒ³ãƒˆãƒ»èª¿æ•™å¸«)
                    driver.get(f"https://s.keibabook.co.jp/chihou/danwa/1/{rid}")
                    danwa_html = driver.page_source
                    meta = get_race_meta(danwa_html)
                    danwa_data = parse_danwa_page(danwa_html)
                    
                    # 3. èª¿æ•™ (ã‚¿ã‚¤ãƒ )
                    driver.get(f"https://s.keibabook.co.jp/chihou/cyokyo/1/{rid}")
                    cyokyo_data = parse_cyokyo_page(driver.page_source)
                    
                    # 4. çµåˆ
                    # å…¨ãƒšãƒ¼ã‚¸ã®é¦¬ç•ªã‚»ãƒƒãƒˆ (ã‚­ãƒ¼ãŒæ–‡å­—åˆ—ã‹æ•°å€¤ã‹æ³¨æ„ã—ã¦ã‚½ãƒ¼ãƒˆ)
                    all_keys = set(syutuba_data.keys()) | set(danwa_data.keys()) | set(cyokyo_data.keys())
                    sorted_umas = sorted(list(all_keys), key=lambda x: int(x) if x.isdigit() else 999)
                    
                    lines = []
                    for u in sorted_umas:
                        # å„è¾æ›¸ã‹ã‚‰get
                        s = syutuba_data.get(u, {})
                        d = danwa_data.get(u, {})
                        c = cyokyo_data.get(u, {})
                        
                        horse = s.get("horse", "ä¸æ˜")
                        jock = s.get("jockey", "ä¸æ˜")
                        change = "ã€âš ï¸ä¹—ã‚Šæ›¿ã‚ã‚Šã€‘" if s.get("is_change") else ""
                        
                        comment = d.get("comment", "ãªã—")
                        trainer = d.get("trainer", "ä¸æ˜") # ã‚³ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸå¸«
                        
                        cyokyo = f"{c.get('tanpyo','')} {c.get('time','')}"
                        
                        lines.append(
                            f"â–¼[é¦¬ç•ª{u}] {horse}\n"
                            f"  é¨æ‰‹: {jock} {change}\n"
                            f"  èª¿æ•™å¸«: {trainer}\n"
                            f"  å©èˆã®è©±: {comment}\n"
                            f"  èª¿æ•™: {cyokyo}"
                        )
                    
                    if not lines:
                        status.warning("ãƒ‡ãƒ¼ã‚¿ãªã—")
                        continue
                        
                    prompt = (
                        f"ãƒ¬ãƒ¼ã‚¹: {meta.get('name','')}\næ¡ä»¶: {meta.get('cond','')}\n\n"
                        "ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿(é¨æ‰‹,èª¿æ•™å¸«,å©èˆã®è©±,èª¿æ•™)ã‹ã‚‰æ¨å¥¨é¦¬ã‚’äºˆæƒ³ã›ã‚ˆã€‚\n"
                        "ç‰¹ã«ã€Œä¹—ã‚Šæ›¿ã‚ã‚Šã€ã®æœ‰ç„¡ã€èª¿æ•™å¸«ã®ã‚³ãƒ¡ãƒ³ãƒˆã®æ„Ÿè§¦ã‚’é‡è¦–ã™ã‚‹ã“ã¨ã€‚\n\n"
                        + "\n".join(lines)
                    )
                    
                    status.info("ğŸ¤– AIåˆ†æä¸­...")
                    ans = ""
                    for chunk in stream_dify(prompt):
                        ans += chunk
                        res_area.markdown(ans + "â–Œ")
                    res_area.markdown(ans)
                    
                    save_history(target_date.year, PLACE_CODE, pname, target_date.month, target_date.day, f"{rnum:02}", rid, ans)
                    status.success("å®Œäº†")
                    
                except Exception as e:
                    status.error(f"Error: {e}")
                
                st.divider()
                
    finally:
        driver.quit()
